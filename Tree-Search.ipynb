{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this notebook we will be implementing a Tree-Search algorithm for mastering a game of Go.\n",
    "\n",
    "OpenAI Gym Environment: https://github.com/aigagror/GymGo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Play Tree\n",
    "\n",
    "First step is to implement a Random Play Tree. In this implementation sequence of moves are organized in tree structure. Any node can be expanded. Random Play Tree choses random possible move at any turn and plays game unless no possible moves left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monte_carlo_tree import RandomPlayTree\n",
    "\n",
    "BOARD_SIZE = 7\n",
    "\n",
    "'''\n",
    "Play a game using random tree strategy\n",
    "'''\n",
    "def random_play():\n",
    "    \n",
    "    tree = RandomPlayTree(BOARD_SIZE)\n",
    "    \n",
    "    root_node = tree.root_node\n",
    "    terminal_node = tree.simulate(root_node)\n",
    "    \n",
    "    return (terminal_node.depth(), tree.evaluate_node(terminal_node))\n",
    "    \n",
    "'''\n",
    "Play a number of random games and display result\n",
    "'''\n",
    "def build_random_play_stats(n_games=100):\n",
    "    \n",
    "    black_wins = 0\n",
    "    white_wins = 0\n",
    "    moves = []\n",
    "    \n",
    "    for _ in range(n_games):\n",
    "        m, winner = random_play()\n",
    "        if winner == 1:\n",
    "            black_wins += 1\n",
    "        else:\n",
    "            white_wins += 1\n",
    "        moves.append(m)\n",
    "    \n",
    "    print(\"Blacks: \", black_wins, \"Whites: \", white_wins, \"Moves mean:\", np.mean(moves))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blacks:  0 Whites:  1 Moves mean: 45.0\n",
      "Wall time: 259 ms\n"
     ]
    }
   ],
   "source": [
    "%time build_random_play_stats(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo Tree Search\n",
    "\n",
    "Then we would sublclass Random Play Tree to implement all methods of Monte Carlo Tree Search algorithm.\n",
    "\n",
    "MCTS involves following stages:\n",
    "\n",
    "### 1. Simulate\n",
    "\n",
    "Simulation is MCTS is a sequence of moves that starts in current node and ends in terminal node.\n",
    "During simulation moves are chosen wrt **rollout policy function** which in usually uniform random.\n",
    "\n",
    "### 2. Expand\n",
    "\n",
    "* Expanded node: a playout has been started in this node\n",
    "* Fully expanded node: if all children of node were visited\n",
    "\n",
    "### 3. Rollout \n",
    "\n",
    "Once node has been expanded result and statistics are propagated all way back to root node \n",
    "through parent nodes.\n",
    "\n",
    "Node Statistics:\n",
    "\n",
    "* Q(v) - Total simulation reward\n",
    "* N(v) - Total number of visits\n",
    "* U(v) - Upper confidence bound\n",
    "\n",
    "### 4. Select \n",
    "\n",
    "UCT is a core of MCTS. It allows us to choose next node among visited nodes.\n",
    "    \n",
    "Q_v/N_v - exploitattion component (favors nodes that were winning)\n",
    "torch.sqrt(torch.log(N_v_parent)/N_v) - exploration component (favors node that weren't visited)\n",
    "c - tradeoff\n",
    "\n",
    "In competetive games Q always computed relative to player who moves.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monte_carlo_tree import MonteCarloPlayTree\n",
    "\n",
    "mcst = MonteCarloPlayTree(BOARD_SIZE)\n",
    "\n",
    "'''\n",
    "Play a game using MonteCarloSearchTree\n",
    "'''\n",
    "def mtsc_play(tree):\n",
    "    \n",
    "    root_node = tree.root_node\n",
    "    terminal_node = tree.simulate(root_node)\n",
    "    \n",
    "    return (terminal_node.depth(), tree.evaluate_node(terminal_node))\n",
    "\n",
    "'''\n",
    "Play a number of random games and display result\n",
    "'''\n",
    "def build_mcst_stats(n_games=100):\n",
    "    \n",
    "    black_wins = 0\n",
    "    white_wins = 0\n",
    "    moves = []\n",
    "    \n",
    "    for counter in range(n_games):\n",
    "        m, winner = mtsc_play(mcst)\n",
    "        if winner == 1:\n",
    "            black_wins += 1\n",
    "        else:\n",
    "            white_wins += 1\n",
    "        moves.append(m)\n",
    "    \n",
    "    print(\"Blacks: \", black_wins, \"Whites: \", white_wins, \"Moves mean:\", np.mean(moves))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blacks:  0 Whites:  1 Moves mean: 101.0\n",
      "Wall time: 5.61 s\n"
     ]
    }
   ],
   "source": [
    "%time build_mcst_stats(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play Random Policy vs Monte Carlo Tree Search Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guided Tree Search\n",
    "\n",
    "Guided Monte Carlo Tree Search augments original algorithm by using neural network to evaluate game states and learn policy. MTSC redefines rollout poliy and UCT functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "\n",
    "    def __init__(self, board_size=BOARD_SIZE):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        \n",
    "        self.board_size = board_size\n",
    "        self.fc = nn.Linear(self.board_size**2, 64)\n",
    "\n",
    "        # Policy head\n",
    "        self.fc_action1 = nn.Linear(64, 32)\n",
    "        self.fc_action2 = nn.Linear(32, self.board_size**2)\n",
    "        \n",
    "        # Critic head\n",
    "        self.fc_value1 = nn.Linear(64, 32)\n",
    "        self.fc_value2 = nn.Linear(32, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        y = x.view(-1, self.board_size**2)\n",
    "        y = F.relu(self.fc(y))\n",
    "        \n",
    "        # Actor head\n",
    "        a = F.relu(self.fc_action1(y))\n",
    "        a = self.fc_action2(a)\n",
    "        \n",
    "        # availability of moves\n",
    "        avail = (torch.abs(x)!=1).float()\n",
    "        avail = avail.view(-1, self.board_size**2) \n",
    "        \n",
    "        # locations where actions are not possible, we set the prob to zero\n",
    "        maxa = torch.max(a)\n",
    "        # subtract off max for numerical stability (avoids blowing up at infinity)\n",
    "        exp = avail*torch.exp(a-maxa)\n",
    "        prob = (exp/torch.sum(exp))\n",
    "        \n",
    "        prob = prob.view(-1, BOARD_SIZE,BOARD_SIZE)\n",
    "        \n",
    "        # Critic head\n",
    "        value = torch.relu(self.fc_value1(y))\n",
    "        value = torch.tanh(self.fc_value2(value))\n",
    "\n",
    "        return prob, value\n",
    "\n",
    "actor_critic_network = ActorCritic().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration #  0\n",
      "number of moves:  10\n",
      "Iteration # 9  loss: tensor(4.9111, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Iteration #  1\n",
      "number of moves:  10\n",
      "Iteration # 9  loss: tensor(4.8278, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Iteration #  2\n",
      "number of moves:  10\n",
      "Iteration # 9  loss: tensor(4.7432, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "Iteration #  3\n"
     ]
    }
   ],
   "source": [
    "from monte_carlo_tree import GuidedMonteCarloPlayTree \n",
    "\n",
    "tree = GuidedMonteCarloPlayTree(BOARD_SIZE, actor_critic_network, device)\n",
    "tree.train(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] https://int8.io/monte-carlo-tree-search-beginners-guide/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
