{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this notebook we will be implementing a Tree-Search algorithm for mastering a game of Go.\n",
    "\n",
    "OpenAI Gym Environment: https://github.com/aigagror/GymGo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Check correctness\n",
    "\n",
    "All basic elements of algorithm are implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Play Tree\n",
    "\n",
    "First step is to implement a Random Play Tree. In this implementation sequence of moves are organized in tree structure. Any node can be expanded. Random Play Tree choses random possible move at any turn and plays game unless no possible moves left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monte_carlo_tree import RandomPlayTree\n",
    "\n",
    "BOARD_SIZE = 4\n",
    "\n",
    "'''\n",
    "Play a game using random tree strategy\n",
    "'''\n",
    "def random_play():\n",
    "    \n",
    "    tree = RandomPlayTree(BOARD_SIZE)\n",
    "    \n",
    "    root_node = tree.root_node\n",
    "    terminal_node = tree.simulate(root_node)\n",
    "    \n",
    "    return (terminal_node.depth(), tree.evaluate_node(terminal_node))\n",
    "    \n",
    "'''\n",
    "Play a number of random games and display result\n",
    "'''\n",
    "def build_random_play_stats(n_games=100):\n",
    "    \n",
    "    black_wins = 0\n",
    "    white_wins = 0\n",
    "    moves = []\n",
    "    \n",
    "    for _ in range(n_games):\n",
    "        m, winner = random_play()\n",
    "        if winner == 1:\n",
    "            black_wins += 1\n",
    "        else:\n",
    "            white_wins += 1\n",
    "        moves.append(m)\n",
    "    \n",
    "    print(\"Blacks: \", black_wins, \"Whites: \", white_wins, \"Moves mean:\", np.mean(moves))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %time build_random_play_stats(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo Tree Search\n",
    "\n",
    "Then we would sublclass Random Play Tree to implement all methods of Monte Carlo Tree Search algorithm.\n",
    "\n",
    "MCTS involves following stages:\n",
    "\n",
    "### 1. Simulate\n",
    "\n",
    "Simulation is MCTS is a sequence of moves that starts in current node and ends in terminal node.\n",
    "During simulation moves are chosen wrt **rollout policy function** which in usually uniform random.\n",
    "\n",
    "### 2. Expand\n",
    "\n",
    "* Expanded node: a playout has been started in this node\n",
    "* Fully expanded node: if all children of node were visited\n",
    "\n",
    "### 3. Rollout \n",
    "\n",
    "Once node has been expanded result and statistics are propagated all way back to root node \n",
    "through parent nodes.\n",
    "\n",
    "Node Statistics:\n",
    "\n",
    "* Q(v) - Total simulation reward\n",
    "* N(v) - Total number of visits\n",
    "* U(v) - Upper confidence bound\n",
    "\n",
    "### 4. Select \n",
    "\n",
    "UCT is a core of MCTS. It allows us to choose next node among visited nodes.\n",
    "    \n",
    "Q_v/N_v - exploitattion component (favors nodes that were winning)\n",
    "torch.sqrt(torch.log(N_v_parent)/N_v) - exploration component (favors node that weren't visited)\n",
    "c - tradeoff\n",
    "\n",
    "In competetive games Q always computed relative to player who moves.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monte_carlo_tree import MonteCarloPlayTree\n",
    "\n",
    "mcst = MonteCarloPlayTree(BOARD_SIZE)\n",
    "\n",
    "'''\n",
    "Play a game using MonteCarloSearchTree\n",
    "'''\n",
    "def mtsc_play(tree):\n",
    "    \n",
    "    root_node = tree.root_node\n",
    "    terminal_node = tree.simulate(root_node)\n",
    "    \n",
    "    return (terminal_node.depth(), tree.evaluate_node(terminal_node))\n",
    "\n",
    "'''\n",
    "Play a number of random games and display result\n",
    "'''\n",
    "def build_mcst_stats(n_games=100):\n",
    "    \n",
    "    black_wins = 0\n",
    "    white_wins = 0\n",
    "    moves = []\n",
    "    \n",
    "    for counter in range(n_games):\n",
    "        m, winner = mtsc_play(mcst)\n",
    "        if winner == 1:\n",
    "            black_wins += 1\n",
    "        else:\n",
    "            white_wins += 1\n",
    "        moves.append(m)\n",
    "    \n",
    "    print(\"Blacks: \", black_wins, \"Whites: \", white_wins, \"Moves mean:\", np.mean(moves))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %time build_mcst_stats(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play Random Policy vs Monte Carlo Tree Search Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guided Tree Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        \n",
    "        self.size = BOARD_SIZE**2\n",
    "        self.conv = nn.Conv2d(1, self.size, kernel_size=2, stride=1, bias=False)\n",
    "        self.fc = nn.Linear(self.size, 32)\n",
    "\n",
    "        # Policy head\n",
    "        self.fc_action1 = nn.Linear(32, 16)\n",
    "        self.fc_action2 = nn.Linear(16, self.size)\n",
    "        \n",
    "        # Critic head\n",
    "        self.fc_value1 = nn.Linear(32, 8)\n",
    "        self.fc_value2 = nn.Linear(8, 1)\n",
    "        self.tanh_value = nn.Tanh()\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        y = F.relu(self.conv(x))\n",
    "        y = y.view(-1, self.size)\n",
    "        y = F.relu(self.fc(y))\n",
    "        \n",
    "        # Policy head\n",
    "        a = F.relu(self.fc_action1(y))\n",
    "        a = self.fc_action2(a)\n",
    "        # availability of moves\n",
    "        avail = (torch.abs(x.squeeze())!=1).float()\n",
    "        avail = avail.view(-1, self.size)        \n",
    "        # locations where actions are not possible, we set the prob to zero\n",
    "        maxa = torch.max(a)\n",
    "        # subtract off max for numerical stability (avoids blowing up at infinity)\n",
    "        exp = avail*torch.exp(a-maxa)\n",
    "        prob = exp/torch.sum(exp)\n",
    "        \n",
    "        # Critic head\n",
    "        value = F.relu(self.fc_value1(y))\n",
    "        value = self.tanh_value(self.fc_value2(value))\n",
    "        return prob[-1].view(BOARD_SIZE,BOARD_SIZE), value\n",
    "\n",
    "actor_critic_network = ActorCritic().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Study\\Reinforcement Learning\\Projects\\Tree-Search\\monte_carlo_tree.py:207: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  result = V_current/N_v + c * np.sqrt(np.log(N_v_parent)/N_v) * node.prob\n",
      "D:\\Study\\Reinforcement Learning\\Projects\\Tree-Search\\monte_carlo_tree.py:207: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  result = V_current/N_v + c * np.sqrt(np.log(N_v_parent)/N_v) * node.prob\n",
      "D:\\Study\\Reinforcement Learning\\Projects\\Tree-Search\\monte_carlo_tree.py:207: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  result = V_current/N_v + c * np.sqrt(np.log(N_v_parent)/N_v) * node.prob\n",
      "D:\\Study\\Reinforcement Learning\\Projects\\Tree-Search\\monte_carlo_tree.py:207: RuntimeWarning: divide by zero encountered in log\n",
      "  result = V_current/N_v + c * np.sqrt(np.log(N_v_parent)/N_v) * node.prob\n",
      "D:\\Study\\Reinforcement Learning\\Projects\\Tree-Search\\monte_carlo_tree.py:207: RuntimeWarning: invalid value encountered in sqrt\n",
      "  result = V_current/N_v + c * np.sqrt(np.log(N_v_parent)/N_v) * node.prob\n"
     ]
    }
   ],
   "source": [
    "from monte_carlo_tree import GuidedMonteCarloPlayTree \n",
    "\n",
    "\n",
    "tree = GuidedMonteCarloPlayTree(BOARD_SIZE, actor_critic_network, device)\n",
    "tree.train(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] https://int8.io/monte-carlo-tree-search-beginners-guide/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
